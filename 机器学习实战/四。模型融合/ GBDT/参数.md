# 1.回归

```python
class sklearn.ensemble.GradientBoostingRegressor(
    loss=’ls’,
    learning_rate=0.1, 
    n_estimators=100, 
    subsample=1.0, 
    criterion=’friedman_mse’, 
    min_samples_split=2, 
    min_samples_leaf=1, 
    min_weight_fraction_leaf=0.0, 
    max_depth=3, 
    min_impurity_decrease=0.0, 
    min_impurity_split=None, 
    init=None, 
    random_state=None, 
    max_features=None, 
    alpha=0.9, 
    verbose=0, 
    max_leaf_nodes=None, 
    warm_start=False, 
    presort=’auto’, 
    validation_fraction=0.1, 
    n_iter_no_change=None, 
    tol=0.0001)
```



| boosting框架参数        | 参数要求                                              | 解释                                                         |
| ----------------------- | ----------------------------------------------------- | ------------------------------------------------------------ |
| **loss**                | {’ls’，’lad’，’huber’，’quantile’}，可选（默认=’ls’） | **分类：**对数似然损失函数“deviance”指数损失函数“exponential”两者输入选择。默认是对数似然损失函数“deviance”。一般来说，推荐使用默认的“deviance”。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法 |
|                         |                                                       | **回归：**有均方差损失(最小二乘回归)“ls”, 绝对误差损失“lad”, Huber损失“huber”和分位数损失“quantile”。默认是均方差“ls”。一般来说，如果数据的噪音点不多，用默认的均方差“ls”比较好。如果是噪音点较多，则推荐用抗噪音的损失函数“huber”。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。 |
| **learning_rate**       | float，optional（默认值= 0.1）                        | 即每个弱学习器的权重缩减系数ν，也称作步长，加上了正则化项，我们的强学习器的迭代公式为$f_k(x)=f_{k−1}(x)+νh_k(x)$。ν的取值范围为0<ν≤1。对于同样的训练集拟合效果，较小的νν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的νν开始调参，默认是1。 |
| **n_estimators**        | int（默认值= 100）                                    | 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是100。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。 |
| **subsample**           | float，optional（默认值= 1.0）                        | 即我们在原理篇的正则化章节讲到的子采样，取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。 |
| **init**                | estimator或’zero’，可选（默认=None）                  | 即我们的初始化的时候的弱学习器，拟合对应原理篇里面的f0(x)，如果不输入，则用训练集样本来做样本集的初始化分类回归预测。否则用init参数提供的学习器做初始化分类回归预测。一般用在我们对数据有先验知识，或者之前做过一些拟合的时候，如果没有的话就不用管这个参数了。`init`必须提供[`fit`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor.fit)和[`predict`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor.predict)。如果为“零”，则初始原始预测设置为零。默认使用`DummyEstimator`，预测平均目标值（对于loss =’ls’）或其他损失的分位数。 |
| **alpha**               | float（默认值= 0.9）                                  | 这个参数只有GradientBoostingRegressor有，当我们使用Huber损失“huber”和分位数损失“quantile”时，需要指定分位数的值。默认是0.9，如果噪音点较多，可以适当降低这个分位数的值。 |
| **criterion**           | string，optional（默认 =“friedman_mse”）              | 衡量分裂质量的功能。支持的标准是弗里德曼的改进得分的均方误差“friedman_mse”，均方误差的“mse”和平均绝对误差的“mae”。默认值“friedman_mse”通常是最好的，因为它可以在某些情况下提供更好的近似值。 |
| **verbose**             | int，默认值：0                                        | 启用详细输出。如果为1则会偶尔打印进度和性能（树越多，频率越低）。如果大于1，则它会打印每棵树的进度和性能。 |
| **warm_start**          | bool，默认值：Fals                                    | 设置`True`为时，重用上一个调用的解决方案以适合并向集合中添加更多估计器，否则，只需擦除以前的解决方案 |
| **presort**             | bool或’auto’，可选（默认=’自动’）                     | 是否预先分配数据以加快拟合中最佳分割的发现。默认情况下，自动模式将使用密集数据上的预分类，并默认对稀疏数据进行常规排序。在稀疏数据上将presort设置为true将引发错误。 |
| **validation_fraction** | float，optional，默认值为0.1                          | 将训练数据的比例留作早期停止的验证集。必须介于0和1之间。仅在`n_iter_no_change`设置为整数时使用。 |
| **n_iter_no_change**    | int，默认None                                         | `n_iter_no_change`用于确定在验证分数没有改善时是否将使用提前停止来终止培训。默认情况下，它设置为“无”以禁用提前停止。如果设置为数字，它将保留`validation_fraction`训练数据的大小作为验证，并在验证得分在所有先前`n_iter_no_change`的迭代次数中没有改善时终止训练。 |
| **tol**                 | float, optional, 默认 1e-4                            | 容忍早期停止。如果损失没有至少改善tol的`n_iter_no_change`迭代次数（如果设置为数字），则训练停止。 |







| 弱学习器参数                 | 参数要求                                            | 解释                                                         |
| ---------------------------- | --------------------------------------------------- | ------------------------------------------------------------ |
| **min_samples_split**        | int, float, optional (default=2)                    | 内部节点再划分所需最小样本数**min_samples_split**: 这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。如果是int，则考虑min_samples_split为最小数量。如果是浮点数，那么它min_samples_split是一个分数， 是每个分割的最小样本数。ceil(min_samples_split * n_samples) |
| **min_samples_leaf**         | int, float, optional (default=1                     | 叶子节点最少样本数**min_samples_leaf**: 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。如果是int，则考虑min_samples_leaf为最小数量。如果是float，那么它min_samples_leaf是一个分数， 是每个节点的最小样本数。ceil(min_samples_leaf * n_samples) |
| **max_depth**                | integer, optional (default=3)                       | 默认可以不输入，如果不输入的话，默认值是3。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。 |
| **min_impurity_decrease**    | float, optional (default=0.)                        | 如果该分裂导致纯度的减少大于或等于该值，则将分裂节点。加权纯度减少方程式如下：N_t  /  N  *  （纯度-  N_t_R  /  N_t  *  right_impurity - N_t_L  /  N_t  *  left_impurity ）N样本总数，N_t是当前节点N_t_L的样本数，左子项中N_t_R的样本数，以及右子项中的样本数。N，N_t，N_t_R并且N_t_L都指的是加权和，如果sample_weight获得通过。 |
| **min_impurity_split**       | float, (default=1e-7)                               | **float，（默认值= 1e-7）**节点划分最小不纯度: 这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动默认值1e-7。 |
| **max_features**             | int, float, string or None, optional (default=None) | **int，float，string或None，可选（默认=无）**划分时考虑的最大特征数max_features: 可以使用很多种类型的值，默认是“None”,意味着划分时考虑所有的特征数；如果是int，则考虑max_features每次拆分时的功能。如果是浮点数，那么它max_features是一个分数，并且 每次拆分时都会考虑特征。int(max_features * n_features)如果是“自动”，那么max_features=n_features。如果是“sqrt”，那么max_features=sqrt(n_features)。如果是“log2”，那么max_features=log2(n_features)。如果没有，那么max_features=n_features。一般来说，如果样本特征数不多，比如小于50，我们用默认的“None”就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。 |
| **min_weight_fraction_leaf** | float, optional (default=0.)                        | 叶子节点最小的样本权重和**min_weight_fraction_leaf**：这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。 |
| **max_leaf_nodes**           | int or None, optional (default=None)                | **int或None，可选（默认=无）**最大叶子节点数: 通过限制最大叶子节点数，可以防止过拟合，默认是“None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。 |



| 属性                     |                                                         | 解释                                                         |
| ------------------------ | ------------------------------------------------------- | ------------------------------------------------------------ |
| **feature_importances_** | array, shape (n_features,)                              | 返回要素重要性（越高，越重要）。除非所有树都是仅由根节点组成的单节点树，否则此数组的值总和为1，在这种情况下，它将是一个零数组。 |
| **oob_improvement_**     | array, shape (n_estimators,)                            | 相对于前一次迭代，袋外样品的损失（=偏差）得到改善。 `oob_improvement_[0]`是`init`估计量的第一阶段损失的改善。 |
| **train_score_**         | array, shape (n_estimators,)                            | **数组，形状（n_estimators，）**第i个分数`train_score_[i]`是`i`在袋内样品上迭代时模型的偏差（=损失）。如果这是对训练数据的偏差。`subsample == 1` |
| **loss_**                | LossFunction                                            | **LossFunction**                                             |
| **init_**                | estimator                                               | **estimator**，提供初始预测的估算器。通过`init`参数或设置`loss.init_estimator`。 |
| **estimators_**          | array of DecisionTreeRegressor, shape (n_estimators, 1) | **DecisionTreeRegressor数组，形状（n_estimators，1）**，拟合子估算器的集合。 |



| 方法                                                        | 参数                                                         | 解释                                                         |
| :---------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| apply(*self,X*)                                             | X : {array-like, sparse matrix}, shape (n_samples, n_features) | 将集合中的树应用于X，返回叶索引。在内部，它的dtype将被转换为 dtype=np.float32。如果提供稀疏矩阵，则将其转换为稀疏矩阵csr_matrix。 |
|                                                             | 返回：X_leaves ： 类似数组，形状（n_samples，n_estimators）  | 对于X中的每个数据点x和整体中的每个树，返回叶x的索引在每个估计器中结束。 |
| fit(*self*, *X*, *y*, *sample_weight=None*, *monitor=None*) |                                                              | 适合梯度增强模型。**X** ： {array-like，sparse matrix}，shape（n_samples，n_features）输入样本。在内部，它将被转换为 dtype=np.float32，并且如果向稀疏提供稀疏矩阵csr_matrix。**y** ： 类似数组，形状（n_samples，）目标值（分类中的字符串或整数，回归中的实数）对于分类，标签必须与类对应。**sample_weight** ： 类似数组，形状（n_samples，）或None，样品权重。如果为None，则样本的权重相等。在每个节点中搜索拆分时，将忽略将创建具有净零或负权重的子节点的拆分。在分类的情况下，如果它们将导致在任一子节点中携带负权重的任何单个类，则也忽略分裂。**monitor** ： 可调用，可选，在每次迭代后使用当前迭代调用监视器，对估计器的引用和_fit_stages作为关键字参数的局部变量 。如果可调用返回，则适配过程停止。监视器可用于各种事物，例如计算延期估计，早期停止，模型内省和快照。callable(i, self, locals()) |
|                                                             | 返回：self： object                                          |                                                              |
| get_params(*self[,deep*)                                    |                                                              | 获取此估算工具的参数，参数：deep ： 布尔值，可选如果为True，将返回此估计器的参数并包含作为估算器的子对象。返回：params ： 将字符串映射到任何字符串，映射到其值的参数名称。 |
| predict(*self,X*)                                           |                                                              | 预测X的回归目标。                                            |
| score(*self,X,y[,sample_weight]*)                           |                                                              | 返回预测的确定系数R ^ 2                                      |
| set_params(*self,params*)                                   |                                                              | 设置此估算器的                                               |
| staged_predict(*self,X*)                                    |                                                              | 预测X的每个阶段的回归目标，参数：X ： {array-like，sparse matrix}，shape（n_samples，n_features）输入样本。在内部，它将被转换为 dtype=np.float32并且如果向稀疏提供稀疏矩阵csr_matrix。返回：y ，形状数组的生成器（n_samples，），输入样本的预测值。 |







# 2.分类

```python
class sklearn.ensemble.GradientBoostingClassifier(
    loss=’deviance’, 
    learning_rate=0.1, 
    n_estimators=100, 
    subsample=1.0, 
    criterion=’friedman_mse’, 
    min_samples_split=2, 
    min_samples_leaf=1, 
    min_weight_fraction_leaf=0.0, 
    max_depth=3, 
    min_impurity_decrease=0.0, 
    min_impurity_split=None, 
    init=None, 
    random_state=None, 
    max_features=None, 
    verbose=0, 
    max_leaf_nodes=None, 
    warm_start=False, 
    presort=’auto’, 
    validation_fraction=0.1, 
    n_iter_no_change=None, 
    tol=0.0001)
```



| boosting框架参数        |                                                      | 解释                                                         |
| ----------------------- | ---------------------------------------------------- | :----------------------------------------------------------- |
| **loss**                | {’deviance’，’exponential’}，可选（默认=’deviance’） | **分类：**对数似然损失函数“deviance”指数损失函数“exponential”两者输入选择。默认是对数似然损失函数“deviance”。一般来说，推荐使用默认的“deviance”。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法 |
| **learning_rate**       | float，optional（默认值= 0.1）                       | 即每个弱学习器的权重缩减系数ν，也称作步长，加上了正则化项，我们的强学习器的迭代公式为$f_k(x)=f_{k−1}(x)+νh_k(x)$。ν的取值范围为0<ν≤1。对于同样的训练集拟合效果，较小的νν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的νν开始调参，默认是1。 |
| **n_estimators**        | int（默认值= 100）                                   | 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。 |
| **subsample**           | float，optional（默认值= 1.0）                       | 即我们在原理篇的正则化章节讲到的子采样，取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。 |
| **init**                | estimator                                            | **estimator或’zero’，可选（默认=无）**即我们的初始化的时候的弱学习器，拟合对应原理篇里面的f0(x)，如果不输入，则用训练集样本来做样本集的初始化分类回归预测。否则用init参数提供的学习器做初始化分类回归预测。一般用在我们对数据有先验知识，或者之前做过一些拟合的时候，如果没有的话就不用管这个参数了。`init`必须提供[`fit`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor.fit)和[`predict`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor.predict)。如果为“零”，则初始原始预测设置为零。默认使用`DummyEstimator`，预测平均目标值（对于loss =’ls’）或其他损失的分位数。 |
| **criterion**           | string，optional（default =“friedman_mse”）          | **string，optional（default =“friedman_mse”）**衡量分裂质量的功能。支持的标准是弗里德曼的改进得分的均方误差“friedman_mse”，均方误差的“mse”和平均绝对误差的“mae”。默认值“friedman_mse”通常是最好的，因为它可以在某些情况下提供更好的近似值。 |
| **verbose**             | int，默认值：0                                       | **int，默认值：0，**启用详细输出。如果为1则会偶尔打印进度和性能（树越多，频率越低）。如果大于1，则它会打印每棵树的进度和性能。 |
| **warm_start**          | bool，默认值：False                                  | **bool，默认值：False，**设置`True`为时，重用上一个调用的解决方案以适合并向集合中添加更多估计器，否则，只需擦除以前的解决方案 |
| **presort**             | bool或’auto’，可选（默认=’自动’）                    | **bool或’auto’，可选（默认=’自动’）**是否预先分配数据以加快拟合中最佳分割的发现。默认情况下，自动模式将使用密集数据上的预分类，并默认对稀疏数据进行常规排序。在稀疏数据上将presort设置为true将引发错误。 |
| **validation_fraction** | float，optional，默认值为0.1                         | **float，optional，默认值为0.1**将训练数据的比例留作早期停止的验证集。必须介于0和1之间。仅在`n_iter_no_change`设置为整数时使用。 |
| **n_iter_no_change**    | int，默认None                                        | **int，默认无**`n_iter_no_change`用于确定在验证分数没有改善时是否将使用提前停止来终止培训。默认情况下，它设置为“无”以禁用提前停止。如果设置为数字，它将保留`validation_fraction`训练数据的大小作为验证，并在验证得分在所有先前`n_iter_no_change`的迭代次数中没有改善时终止训练。 |
| **tol**                 | float，optional，默认值1e-4                          | **float，optional，默认值1e-4**容忍早期停止。如果损失没有至少改善tol的`n_iter_no_change`迭代次数（如果设置为数字），则训练停止。 |







| 弱学习器参数                 | 解释                                                         |
| ---------------------------- | ------------------------------------------------------------ |
| **min_samples_split**        | 内部节点再划分所需最小样本数**min_samples_split**: 这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。如果是int，则考虑min_samples_split为最小数量。如果是浮点数，那么它min_samples_split是一个分数， 是每个分割的最小样本数。ceil(min_samples_split * n_samples) |
| **min_samples_leaf**         | 叶子节点最少样本数**min_samples_leaf**: 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。如果是int，则考虑min_samples_leaf为最小数量。如果是float，那么它min_samples_leaf是一个分数， 是每个节点的最小样本数。ceil(min_samples_leaf * n_samples) |
| **max_depth**                | 默认可以不输入，如果不输入的话，默认值是3。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。 |
| **min_impurity_decrease**    | **float，optional（默认= 0）**如果该分裂导致纯度的减少大于或等于该值，则将分裂节点。加权纯度减少方程式如下：N_t  /  N  *  （纯度-  N_t_R  /  N_t  *  right_impurity - N_t_L  /  N_t  *  left_impurity ）N样本总数，N_t是当前节点N_t_L的样本数，左子项中N_t_R的样本数，以及右子项中的样本数。N，N_t，N_t_R并且N_t_L都指的是加权和，如果sample_weight获得通过。 |
| **min_impurity_split**       | **float，（默认值= 1e-7）**节点划分最小不纯度: 这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动默认值1e-7。 |
| **max_features**             | **int，float，string或None，可选（默认=无）**划分时考虑的最大特征数max_features: 可以使用很多种类型的值，默认是“None”,意味着划分时考虑所有的特征数；如果是int，则考虑max_features每次拆分时的功能。如果是浮点数，那么它max_features是一个分数，并且 每次拆分时都会考虑特征。int(max_features * n_features)如果是“自动”，那么max_features=n_features。如果是“sqrt”，那么max_features=sqrt(n_features)。如果是“log2”，那么max_features=log2(n_features)。如果没有，那么max_features=n_features。一般来说，如果样本特征数不多，比如小于50，我们用默认的“None”就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。 |
| **min_weight_fraction_leaf** | 叶子节点最小的样本权重和**min_weight_fraction_leaf**：这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。 |
| **max_leaf_nodes**           | **int或None，可选（默认=无）**最大叶子节点数: 通过限制最大叶子节点数，可以防止过拟合，默认是“None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。 |



| 属性                     |                                                              | 解释                                                         |
| ------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **feature_importances_** | array, shape (n_features,)                                   | **数组，形状（n_features，）**返回要素重要性（越高，越重要）。除非所有树都是仅由根节点组成的单节点树，否则此数组的值总和为1，在这种情况下，它将是一个零数组。 |
| **oob_improvement_**     | array, shape (n_estimators,)                                 | **数组，形状（n_estimators，）**相对于前一次迭代，袋外样品的损失（=偏差）得到改善。 `oob_improvement_[0]`是`init`估计量的第一阶段损失的改善。 |
| **train_score_**         | array, shape (n_estimators,)                                 | **数组，形状（n_estimators，）**第i个分数`train_score_[i]`是`i`在袋内样品上迭代时模型的偏差（=损失）。如果这是对训练数据的偏差。`subsample == 1` |
| **loss_**                | LossFunction                                                 | **LossFunction**                                             |
| **init_**                | estimator                                                    | **estimator**，提供初始预测的估算器。通过`init`参数或设置`loss.init_estimator`。 |
| **estimators_**          | ndarray of DecisionTreeRegressor,shape (n_estimators, `loss_.K`) | **DecisionTreeRegressor数组，形状（n_estimators，1）**，拟合子估算器的集合。 |



| 方法                                 | 解释                                                         |
| ------------------------------------ | ------------------------------------------------------------ |
| apply(self,X)                        | 将集合中的树应用于X，返回叶索引。参数：X ： {array-like，sparse matrix}，shape（n_samples，n_features）输入样本。在内部，它的dtype将被转换为 dtype=np.float32。如果提供稀疏矩阵，则将其转换为稀疏矩阵csr_matrix。返回：X_leaves ： 类似数组，形状（n_samples，n_estimators）对于X中的每个数据点x和整体中的每个树，返回叶x的索引在每个估计器中结束。 |
| fit(self,X,y[sample_weight,monitor]) | 适合梯度增强模型。**X** ： {array-like，sparse matrix}，shape（n_samples，n_features）输入样本。在内部，它将被转换为 dtype=np.float32，并且如果向稀疏提供稀疏矩阵csr_matrix。**y** ： 类似数组，形状（n_samples，）目标值（分类中的字符串或整数，回归中的实数）对于分类，标签必须与类对应。**sample_weight** ： 类似数组，形状（n_samples，）或None，样品权重。如果为None，则样本的权重相等。在每个节点中搜索拆分时，将忽略将创建具有净零或负权重的子节点的拆分。在分类的情况下，如果它们将导致在任一子节点中携带负权重的任何单个类，则也忽略分裂。**monitor** ： 可调用，可选，在每次迭代后使用当前迭代调用监视器，对估计器的引用和_fit_stages作为关键字参数的局部变量 。如果可调用返回，则适配过程停止。监视器可用于各种事物，例如计算延期估计，早期停止，模型内省和快照。callable(i, self, locals())True |
| get_params(self[,deep)               | 获取此估算工具的参数，参数：deep ： 布尔值，可选如果为True，将返回此估计器的参数并包含作为估算器的子对象。返回：params ： 将字符串映射到任何字符串，映射到其值的参数名称。 |
| predict(self,X)                      | 预测X的回归目标。                                            |
| score(self,X,y[,sample_weight])      | 返回预测的确定系数R ^ 2                                      |
| set_params(self,params)              | 设置此估算器的                                               |
| staged_predict(self,X)               | 预测X的每个阶段的回归目标，参数：X ： {array-like，sparse matrix}，shape（n_samples，n_features）输入样本。在内部，它将被转换为 dtype=np.float32并且如果向稀疏提供稀疏矩阵csr_matrix。返回：y ，形状数组的生成器（n_samples，），输入样本的预测值。 |























































