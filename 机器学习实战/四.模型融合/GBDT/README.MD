# 一、简介

　　GBDT是Gradient Boosting Decison Tree的简称，其中Gradient是梯度，是这个方法的核心；Boosting是提升，是这个方法的框架；Decision Tree是决策树，是实现这个方法用到的模型。

　　GBDT可以解决**回归问题**，经过一些处理也可以解决**分类(二类、多类)问题**，但是用到的树都是**回归树**，这一点需要牢记。

　　首先通过简单的回归例子说明一下提升(Boosting)树：

```
 举个例子，如果样本1的输出真实值为10，树T1针对样本1的预测值为18，
 然后我们让树T2去拟合样本1的值为10-18=-8(残差)。如果树T2的输出值为-10，
 我们再让树T3去拟合-8-(-10)=2(残差)，结果树T3的预测值为1。
 如果到此迭代结束，在最终对样本1的预测值为：18+(-10)+1=9。
```

　　到这里，提升回归树的流程就大致清楚了。也就是通过多轮迭代，每轮迭代产生一个弱模型，每个模型都是在上一个模型的残差基础上进行训练的，最后将所有树的结果**求和**得出最终的结果。

　　GBDT就是在提升树的基础上，利用了梯度提升的方法，也就是用损失函数的负梯度在当前模型下的值来作为提升树中残差的近似值。对于GBDT用于回归问题而言，如果损失函数定义为MSE，则其负梯度就是残差。因此残差是损失函数负梯度的一种特殊情况。负梯度是残差这种思想的一般化。残差只可以用于回归问题，但是这种负梯度的思想也可用于分类问题。

**几个误区：**

- **树的个数就是最大迭代次数，与分类类别无关**
- **实际计算的残差 = 样本i对应类别k的真实概率 - 上一轮预测的概率**
- **多分类问题每棵树都是多类别分类器**
- GBDT算法基树采用CART回归树（**回归、分析用的都是回归树**），树节点的划分指标是平方损失函数，叶子节点的值是落在该叶子节点所有样本的目标均值。树与树之间的Boosting逻辑是：新树拟合的目标是上一课树的损失函数的负梯度的值。GBDT最终的输出结果是将样本在所有树上的叶子值相加。可以看出，GBDT最核心的两部分，Boosting 提升说明每棵树之间是有关系有序的、Gradient梯度指明了提升的方向与大小。

# 二、GBDT步骤

![img](https://img2018.cnblogs.com/blog/1252882/201905/1252882-20190527145148281-800982085.png)



## 1.回归问题

- **训练数据集**

　　　　[![Data0 = \{(X1,Y1), (X2,Y2),\cdots ,(Xn,Yn)\}](https://camo.githubusercontent.com/3651931612a99ad67421e9cfda5b5dbfd4601dbf/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f44617461302673706163653b3d2673706163653b5c7b2858312c5931292c2673706163653b2858322c5932292c5c63646f74732673706163653b2c28586e2c596e295c7d)](https://www.codecogs.com/eqnedit.php?latex=Data0&space;=&space;\{(X1,Y1),&space;(X2,Y2),\cdots&space;,(Xn,Yn)\})

　　　　Y0为输出值序列，X0为输入的向量集合。

- 定义损失函数

    - **a.MSE(GBDT回归所用)**  

        ![img](https://camo.githubusercontent.com/669d1b0d4bb10c2625cbd2476df9c1a81ca39bf7/68747470733a2f2f696d67323031382e636e626c6f67732e636f6d2f626c6f672f313235323838322f3230313930352f313235323838322d32303139303532373134353630383630342d313139363833353630322e706e67)
        

    - **b.绝对损失**

        ![img](https://camo.githubusercontent.com/919c04b86d20ba0e9175ff90dc4719e154adc5e8/68747470733a2f2f696d67323031382e636e626c6f67732e636f6d2f626c6f672f313235323838322f3230313930352f313235323838322d32303139303532373134353730303330312d313836383631333537352e706e67)
        



    - **c.Huber损失**　　　

　　　　　　它是MSE和绝对损失的组合形式，对于远离中心的异常点，采用绝对损失，其他的点采用MSE。

　　　　　　这个界限一般用分位数点度量。公式如下

　　　　　 ![img](https://img2018.cnblogs.com/blog/1252882/201905/1252882-20190527145816330-818038704.png)
      

- **初始化：针对数据集Data0，建立第一个CART回归树T0**

　　　　CART回归树T0，说是树，其实就只包括一个叶子节点。

　　　　这个树的输出值就是使得损失函数值最小的数r，该树对于训练数据集的序列定义为Y_T0，

　　　　其实这个序列的元素全是r：

　　　　　　![\\Cost(Y0, \gamma) \\s.t.\mathbf{min}(Y0)\leq \gamma \leq \mathbf{max}(Y0)](https://camo.githubusercontent.com/4fa7d087f8385b4f1c28ea95dff8aca19c52c778/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f5c5c436f73742859302c2673706163653b5c67616d6d61292673706163653b5c5c732e742e5c6d61746862667b6d696e7d285930295c6c65712673706163653b5c67616d6d612673706163653b5c6c65712673706163653b5c6d61746862667b6d61787d28593029)

　　　　**当前模型为F_0(x) = Y_T0**

- 计算损失函数梯度

    下面分别给出三种损失函数的负梯度在当前模型Y_T0下的值：

    - **a.MSE**

        ![G\_i = - \frac{\partial \mathbf{Cost(Y0, Y\_T0)}}{\partial Y\_T0\_i} = Y0\_i - Y\_T0\_i](https://camo.githubusercontent.com/1fc62f3b7ad95a75908bf8e53810506958f52281/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f475c5f692673706163653b3d2673706163653b2d2673706163653b5c667261637b5c7061727469616c2673706163653b5c6d61746862667b436f73742859302c2673706163653b595c5f5430297d7d7b5c7061727469616c2673706163653b595c5f54305c5f697d2673706163653b3d2673706163653b59305c5f692673706163653b2d2673706163653b595c5f54305c5f69)

    - **b.MAE**

        ![G\_i = -\frac{\partial \mathbf{Cost(Y0, Y\_T0)}}{\partial Y\_T0\_i} = \mathbf{sign}(Y0\_i - Y\_T0\_i)](https://camo.githubusercontent.com/20a70fa9d37dfa17b4576d813f98b225c7e2540c/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f475c5f692673706163653b3d2673706163653b2d5c667261637b5c7061727469616c2673706163653b5c6d61746862667b436f73742859302c2673706163653b595c5f5430297d7d7b5c7061727469616c2673706163653b595c5f54305c5f697d2673706163653b3d2673706163653b5c6d61746862667b7369676e7d2859305c5f692673706163653b2d2673706163653b595c5f54305c5f6929)

    - **c.Huber损失**

　　　　　　![G\_i = - \frac{\partial \mathbf{Cost(Y0, Y\_T0)}}{\partial Y\_T0\_i} = \left\{\begin{matrix} Y0\_i - Y\_T0\_i,|Y0\_i - Y\_T0\_i|\leq \delta \\ \\ \delta *\mathbf{sign}(Y0\_i - Y\_T0\_i),|Y0\_i - Y\_T0\_i|> \delta \end{matrix}\right.](https://camo.githubusercontent.com/9b0bd90f314280e339e8d66fcd091132ce857695/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f475c5f692673706163653b3d2673706163653b2d2673706163653b5c667261637b5c7061727469616c2673706163653b5c6d61746862667b436f73742859302c2673706163653b595c5f5430297d7d7b5c7061727469616c2673706163653b595c5f54305c5f697d2673706163653b3d2673706163653b5c6c6566745c7b5c626567696e7b6d61747269787d2673706163653b59305c5f692673706163653b2d2673706163653b595c5f54305c5f692c7c59305c5f692673706163653b2d2673706163653b595c5f54305c5f697c5c6c65712673706163653b5c64656c74612673706163653b5c5c2673706163653b5c5c2673706163653b5c64656c74612673706163653b2a5c6d61746862667b7369676e7d2859305c5f692673706163653b2d2673706163653b595c5f54305c5f69292c7c59305c5f692673706163653b2d2673706163653b595c5f54305c5f697c3e2673706163653b5c64656c74612673706163653b5c656e647b6d61747269787d5c72696768742e)

- **生成新的数据集Data1**　　

　　　　**![img](https://img2018.cnblogs.com/blog/1252882/201905/1252882-20190527152744716-1638577962.png)**

- **建立CART回归树T1**

　　　　针对新生成的数据集Data1，建立CART回归树T1。

　　　　回归树T1建立完成后，每个叶子节点的输出值不再设定为平均值，

　　　　而是利用和初始化时相同的方法，对每一个叶子节点，

　　　　计算使得下式具有最小值的值作为该叶子节点的输出值：

　　　　下面以叶子节点h为例。

　　　　![\\Cost(Y0\_T1\_h, \gamma + c\_T1\_h)](https://camo.githubusercontent.com/c07e6919609541b67a131ee33a87218216de1a8c/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f5c5c436f73742859305c5f54315c5f682c2673706163653b5c67616d6d612673706163653b2b2673706163653b635c5f54315c5f6829)

　　　　其中Y0_T1_h 表示树T1的叶子节点h中包含的样本的真实输出值，

　　　　c_T1_h为树T1的叶子节点h的最佳的输出值。训练数据集总的输出定义为c_T1

　　　　上述函数中前面的项是样本的真实输出值，这一点是不变的。

　　　　后面的项就是目前所有模型的集成值。

　　　　**当前模型为F_1(x) = Y_T0 + c_T1**

- **迭代**

　　　   计算损失函数的负梯度在当前模型F_1(x)下的值，然后更新训练数据集的输出值，

　　　　然后再建立回归树，再计算每个叶子节点的最佳输出，

　　　　然后得到当前模型F_2(X)，以此迭代，直到满足条件结束。

- **结果集成**　　

　　　　假设对应m个弱模型的预测的结果分别为Pre_1, Pre_2,……,Pre_m, 将所有回归树的结果相加，

　　　　即为最终的结果，如下公式。

　　　　　　![Pre(m) = \sum_{j=1}^{m}Pre\_j\\ Pre(k) = Pre(k-1)+Pre\_k](https://camo.githubusercontent.com/46c4e4a903c2634dbaedf57eea0a69fcfddaba76/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f507265286d292673706163653b3d2673706163653b5c73756d5f7b6a3d317d5e7b6d7d5072655c5f6a5c5c2673706163653b507265286b292673706163653b3d2673706163653b507265286b2d31292b5072655c5f6b)

　　　　为了防止过拟合，可添加正则化的环节，

　　　　　　![Pre(k) = Pre(k-1)+\alpha *Pre\_k, k\geq 2,0<\alpha \leq 1](https://camo.githubusercontent.com/8ce18daebd37541fb7fce34799aec84803c9af9b/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f507265286b292673706163653b3d2673706163653b507265286b2d31292b5c616c7068612673706163653b2a5072655c5f6b2c2673706163653b6b5c6765712673706163653b322c303c5c616c7068612673706163653b5c6c65712673706163653b31)，a为正则化因子。

 

## 2.分类问题

　　这里解释下，为什么GBDT解决分类问题用的也是回归树。因为GBDT的根本就是在不断地拟合残差，残差的加减可以逐渐减小偏差。而分类树输出的是类别，他们之间的加减是没有意义的。**分类问题和回归问题的最大不同在于损失函数的定义以及节点输出值的确定**，其他都是一样的。

### 2.1 二分类

- **对数似然损失函数**

    [![\\ \mathbf{Cost(Y\_r, F(X))} = \mathbf{log}(1+\mathbf{exp}(-2*\mathbf{Y\_r}*\mathbf{F(X)})), \mathbf{Y\_r}=1 \mathbf{or}-1\\ \\ \mathbf{F(X)} = \frac{1}{2}\mathbf{log}(\frac{P(Y=1|X)}{P(Y=-1|X)})](https://camo.githubusercontent.com/85df3e88a7dfa398a0a975e90dc97e4e6c136ac7/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f5c5c2673706163653b5c6d61746862667b436f737428595c5f722c2673706163653b46285829297d2673706163653b3d2673706163653b5c6d61746862667b6c6f677d28312b5c6d61746862667b6578707d282d322a5c6d61746862667b595c5f727d2a5c6d61746862667b462858297d29292c2673706163653b5c6d61746862667b595c5f727d3d312673706163653b5c6d61746862667b6f727d2d315c5c2673706163653b5c5c2673706163653b5c6d61746862667b462858297d2673706163653b3d2673706163653b5c667261637b317d7b327d5c6d61746862667b6c6f677d285c667261637b5028593d317c58297d7b5028593d2d317c58297d29)](https://www.codecogs.com/eqnedit.php?latex=\\&space;\mathbf{Cost(Y\_r,&space;F(X))}&space;=&space;\mathbf{log}(1+\mathbf{exp}(-2*\mathbf{Y\_r}*\mathbf{F(X)})),&space;\mathbf{Y\_r}=1&space;\mathbf{or}-1\\&space;\\&space;\mathbf{F(X)}&space;=&space;\frac{1}{2}\mathbf{log}(\frac{P(Y=1|X)}{P(Y=-1|X)}))

    其中：

    [![F\_0(X) = \frac{1}{2}\mathbf{log}(\frac{1+\mathbf{avg}(\mathbf{Y\_r})}{1-\mathbf{avg}(\mathbf{Y\_r})})](https://camo.githubusercontent.com/17ef5ecbd10c31bc3d43eeabf7fadad39189e7ea/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f465c5f302858292673706163653b3d2673706163653b5c667261637b317d7b327d5c6d61746862667b6c6f677d285c667261637b312b5c6d61746862667b6176677d285c6d61746862667b595c5f727d297d7b312d5c6d61746862667b6176677d285c6d61746862667b595c5f727d297d29)](https://www.codecogs.com/eqnedit.php?latex=F\_0(X)&space;=&space;\frac{1}{2}\mathbf{log}(\frac{1+\mathbf{avg}(\mathbf{Y\_r})}{1-\mathbf{avg}(\mathbf{Y\_r})}))

- **计算负梯度**

    [![\\G\_i = -\frac{\partial \mathbf{Cost(Y\_r, F(X))}}{\partial F(X\_i)} = \frac{\mathbf{2Y\_r\_i}}{1 + \mathbf{exp}(\mathbf{2Y\_r\_i}*F(X\_i))}](https://camo.githubusercontent.com/675a7b07a89f4fd19880b8083f9077a89adebbb3/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f5c5c475c5f692673706163653b3d2673706163653b2d5c667261637b5c7061727469616c2673706163653b5c6d61746862667b436f737428595c5f722c2673706163653b46285829297d7d7b5c7061727469616c2673706163653b4628585c5f69297d2673706163653b3d2673706163653b5c667261637b5c6d61746862667b32595c5f725c5f697d7d7b312673706163653b2b2673706163653b5c6d61746862667b6578707d285c6d61746862667b32595c5f725c5f697d2a4628585c5f6929297d)](https://www.codecogs.com/eqnedit.php?latex=\\G\_i&space;=&space;-\frac{\partial&space;\mathbf{Cost(Y\_r,&space;F(X))}}{\partial&space;F(X\_i)}&space;=&space;\frac{\mathbf{2Y\_r\_i}}{1&space;+&space;\mathbf{exp}(\mathbf{2Y\_r\_i}*F(X\_i))})

- **计算树的叶子节点的最佳的输出值**

    对于树Ta的第h个叶子节点，其输出值为：

    ![img](https://camo.githubusercontent.com/b7d2a2e16e65efbd0781f8108da99e59cb19671a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f435c5f54615c5f682673706163653b3d2673706163653b5c667261637b2673706163653b5c73756d5f7b695c73756273657465712673706163653b54615c5f687d2673706163653b475c5f695c5f54615c5f687d7b2673706163653b5c73756d5f7b695c73756273657465712673706163653b54615c5f687d5b7c2673706163653b475c5f695c5f54615c5f687c2a28322d7c475c5f695c5f54615c5f687c295d7d)

- **最终的类别**

    迭代完成后，得到的模型为F_m，对于样本X，分别计算其属于1和-1的概率：

    [![\\P(Y=1|X)= \frac{1}{1+exp(-2F\_m(X))}\\ \\ P(Y=-1|X)= \frac{1}{1+exp(2F\_m(X))}](https://camo.githubusercontent.com/9518c15a8df7918bc56eb1786b877257646ce183/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f5c5c5028593d317c58293d2673706163653b5c667261637b317d7b312b657870282d32465c5f6d285829297d5c5c2673706163653b5c5c2673706163653b5028593d2d317c58293d2673706163653b5c667261637b317d7b312b6578702832465c5f6d285829297d)](https://www.codecogs.com/eqnedit.php?latex=\\P(Y=1|X)=&space;\frac{1}{1+exp(-2F\_m(X))}\\&space;\\&space;P(Y=-1|X)=&space;\frac{1}{1+exp(2F\_m(X))})

    概率较大的对应的类别就是预测类别。

### 2.2 多分类 

　　对于多分类任务，GDBT采用OneVsOther，也就是对每个类别训练m个弱模型。假设有K个类别，那么训练完之后总共有K*m颗树。

- **多分类对数似然损失函数**

    [![\\ \mathbf{Cost(Y\_r,F(X))}=-\sum_{k=1}^{K}Y\_r\_k*\mathbf{log}p\_k(X) \\ p\_k(X)=\frac{e^{F\_k(X)}}{\sum_{l=1}^{K}e^{F\_l(X)}}](https://camo.githubusercontent.com/1d0ed7ffe30e5c5887342ce3aa18deeb1763d213/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f5c5c2673706163653b5c6d61746862667b436f737428595c5f722c46285829297d3d2d5c73756d5f7b6b3d317d5e7b4b7d595c5f725c5f6b2a5c6d61746862667b6c6f677d705c5f6b2858292673706163653b5c5c2673706163653b705c5f6b2858293d5c667261637b655e7b465c5f6b2858297d7d7b5c73756d5f7b6c3d317d5e7b4b7d655e7b465c5f6c2858297d7d)](https://www.codecogs.com/eqnedit.php?latex=\\&space;\mathbf{Cost(Y\_r,F(X))}=-\sum_{k=1}^{K}Y\_r\_k*\mathbf{log}p\_k(X)&space;\\&space;p\_k(X)=\frac{e^{F\_k(X)}}{\sum_{l=1}^{K}e^{F\_l(X)}})

- **计算负梯度**

    因为每次迭代都需要对每一个类别都构建一个回归树F_k(X), k=1,2,……，K。

    因此每个样本都要计算k次的负梯度，只不过每一次针对的类别不同。

    [![G\_k\_i = \mathbf{Y\_r}\_i - p\_k\_m(X\_i)](https://camo.githubusercontent.com/2a2c80e91b1d80c4dca778d50c3ac85bc883a5f1/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f475c5f6b5c5f692673706163653b3d2673706163653b5c6d61746862667b595c5f727d5c5f692673706163653b2d2673706163653b705c5f6b5c5f6d28585c5f6929)](https://www.codecogs.com/eqnedit.php?latex=G\_k\_i&space;=&space;\mathbf{Y\_r}\_i&space;-&space;p\_k\_m(X\_i))

- **计算树的叶子节点的最佳的输出值**

    ![img](https://camo.githubusercontent.com/2780d2b63877ab5cfa71e7beb410096d6170ee54/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f435c5f54615c5f682673706163653b3d2673706163653b5c667261637b4b2d317d7b4b7d5c667261637b2673706163653b5c73756d5f7b695c73756273657465712673706163653b54615c5f687d2673706163653b475c5f695c5f54615c5f687d7b2673706163653b5c73756d5f7b695c73756273657465712673706163653b54615c5f687d5b7c2673706163653b475c5f695c5f54615c5f687c2a28322d7c475c5f695c5f54615c5f687c295d7d)

- **最终的类别**

    到最后，每个类别都会得到m个回归树，将每个类别回归树的结果集成起来，会得到K个最终的集成结果。

    对于一个样本而言，会相应的得到K个结果，然后按照Softmax的方式，得到这个样本最终的类别。

 

# 三、GBDT总结

## 1.优缺点

**优点：**

- 可以灵活处理各种类型的数据，包括连续值和离散值。
- 在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。
- 使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。

**缺点：**由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。

 

## 2.GBDT 和 随机森林/Adaboost/LR的区别与联系

- ### GBDT和随机森林

    - 相同点：
        - 都是由多棵树组成 
        - 最终的结果都是由多棵树一起决定
    - 不同点：
        - 组成随机森林的树可以并行生成；而GBDT只能是串行生成 ； 
        - 对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来 ； 
        - 随机森林对异常值不敏感，GBDT对异常值非常敏感 ； 
        - 随机森林对训练集一视同仁，GBDT是基于权值的弱分类器的集成 ； 
        - 随机森林是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能；
        - 随机森林既可以使用决策树也可以使用回归树，但是gbdt采用的都是CART回归树。

- ### GBDT和Adaboost

　　和AdaBoost一样，Gradient Boosting也是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。不同的是，AdaBoost是通过**提升错分数据点的权重**来定位模型的不足而Gradient Boosting是通过**算梯度**（gradient）来定位模型的不足。因此相比AdaBoost, Gradient Boosting可以使用更多种类的目标函数,而当目标函数是均方误差时，计算损失函数的负梯度值在当前模型的值即为残差。

- ### GBDT和LR

　　从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是**一条曲线**，而GBDT的决策边界可能是**很多条线**。

 
